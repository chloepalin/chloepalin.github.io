 # Lesson 1 of the Fast.ai Course: A Journey into Practical Deep Learning
 
 Today I'll be sharing my insights and experiences from Lesson 1 
 of the Fast.ai course by Jeremy Howard. In this lesson, Jeremy takes us on a practical 
 and intuitive exploration of deep learning, using the Fast.ai library as our guide.

## Data Acquisition Process

Lesson 1 takes a top-down approach to explain image classification using convolutional neural networks (CNNs), which we will get to later. What I like most about this course is that you learn  by doing - which does seem academically strange not learning the math behind everything first, but I am up for getting stuck straight into it! Jeremy takes us through the process of building and training a model using the Fast.ai library for creating a bird recongiser.

Let's take a look at a code snippet and break down each step:

```
 searches = 'forest', 'bird'
 path = Path('bird_or_not')
 from time import sleep

 for o in searches:
     dest = (path/o)
     dest.mkdir(exist_ok=True, parents=True)
     download_images(dest, urls=search_images(f'{o} photo'))
     sleep(10)  # Pause between searches to avoid over-loading server
     download_images(dest, urls=search_images(f'{o} sun photo'))
     sleep(10)
     download_images(dest, urls=search_images(f'{o} shade photo'))
     sleep(10)

```

This code snippet showcases an essential part of the data acquisition process. Firstly, define the search terms, such as 'forest' and 'bird,' and set the path where our downloaded images will be stored. Using the search_images() and download_images() functions, we can then conduct multiple searches to gather a diverse collection of images related to what we want to search for. This method simplifies the process of obtaining labelled data, a crucial step in training our deep learning models.

## Simplifying Data Loading and Transformation

Next, we learnt about the powerful DataBlock API in Fast.ai, which simplifies the data loading. Let's explore another code snippet that showcases the DataBlock functionality:

```
dls = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    item_tfms=[Resize(192, method='squish')]
).dataloaders(path)

dls.show_batch(max_n=6)
```
The above code demonstrates how we can define a DataBlock to preprocess and load our data for training. By specifying the blocks as `ImageBlock` and `CategoryBlock`, we can then define the types of data we are working with. The get_items() function retrieves the file paths of our image data, and the splitter determines how the dataset will be split into training and validation sets. In this case, we are using `RandomSplitter` that assigns 20% of the data to the validation set. The `seed=42` ensures reproducibility of the split.

We also define the get_y function, which extracts the labels for our images. In this case, it retrieves the parent folder name as the category/label. Furthermore, we apply the Resize transformation to each image, ensuring a consistent size for our input data.

Once the DataBlock is defined, we create the data loaders using the specified path to the dataset directory. We can then visualize a batch of the data using the show_batch method.





## Data Cleaning and Preprocessing

## 

